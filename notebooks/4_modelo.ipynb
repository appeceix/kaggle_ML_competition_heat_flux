{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Este es el cuarto modelo. IDÉNTICO AL PRIMERO PERO AÑADIENDO ENSAMBLER. Incluye:\n",
    "#### - Malla de parámetros.\n",
    "#### - Datos de \"Primer procesado de datos\": sin variables categóricas y nan rellenos con media/moda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Exportar modelos\n",
    "import pickle\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# CV\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Modelos\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Métricas\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos datos\n",
    "data = pd.read_csv('../data/processed/primer_procesado_entrenamiento.csv', sep = '\\t')\n",
    "target = pd.read_csv('../data/processed/primer_procesado_target.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiamos los nombres de las features para que no contengan espacios ni caracteres especiales\n",
    "data.columns = ['pressure', 'mass_flux', 'x_e_out', 'D_e', 'D_h', 'length', 'chf_exp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos en features y target, siendo el target x_e_out [-]\n",
    "X = data.drop(['x_e_out'], axis = 1)\n",
    "y = data['x_e_out']\n",
    "# Luego tendremos que recuperar los nombres originales de las features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos entre train y test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un primer pie cualquier clasificador. Irá cambiando según va probando pero necesita 1.\n",
    "pipe = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('estimator', Ridge())]\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una malla de parámetros para cada modelo a probar\n",
    "\n",
    "param_grid_linear_regression = {\n",
    "    'estimator' : [LinearRegression()],\n",
    "    'estimator__fit_intercept': [True, False],  # Probar con o sin intercepto\n",
    "    'estimator__normalize': [True, False]  # Probar con o sin normalización de características\n",
    "}\n",
    "\n",
    "param_grid_ridge = {\n",
    "    'estimator' : [Ridge()],\n",
    "    'estimator__alpha': [0.1, 1.0, 10.0],  # Valores de alpha a probar\n",
    "    'estimator__solver': ['svd', 'cholesky', 'lsqr']  # Métodos de solución a probar\n",
    "}\n",
    "\n",
    "param_grid_lasso = {\n",
    "    'estimator' : [Lasso()],\n",
    "    'estimator__alpha': [0.1, 1.0, 10.0],  # Valores de alpha a probar\n",
    "    'estimator__max_iter': [1000, 2000, 5000]  # Número máximo de iteraciones a probar\n",
    "}\n",
    "\n",
    "param_grid_elasticnet = {\n",
    "    'estimator' : [ElasticNet()],\n",
    "    'estimator__alpha': [0.1, 1.0, 10.0],  # Valores de alpha a probar\n",
    "    'estimator__l1_ratio': [0.25, 0.5, 0.75],  # Valores de l1_ratio a probar\n",
    "    'estimator__max_iter': [1000, 2000, 5000]  # Número máximo de iteraciones a probar\n",
    "}\n",
    "\n",
    "param_grid_svr = {\n",
    "    'estimator' : [SVR()],\n",
    "    'estimator__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  # Kernels a probar\n",
    "    'estimator__C': [0.1, 1.0, 10.0],  # Valores de C a probar\n",
    "    'estimator__epsilon': [0.01, 0.1, 1.0]  # Valores de epsilon a probar\n",
    "}\n",
    "\n",
    "param_grid_decision_tree = {\n",
    "    'estimator' : [DecisionTreeRegressor()],\n",
    "    'estimator__max_depth': [None, 5, 10, 20],  # Profundidad máxima del árbol a probar\n",
    "    'estimator__min_samples_split': [2, 5, 10],  # Número mínimo de muestras requeridas para dividir un nodo interno\n",
    "    'estimator__min_samples_leaf': [1, 2, 4]  # Número mínimo de muestras requeridas en un nodo hoja\n",
    "}\n",
    "\n",
    "param_grid_random_forest = {\n",
    "    'estimator' : [RandomForestRegressor()],\n",
    "    'estimator__n_estimators': [10, 50, 100],  # Número de árboles a probar\n",
    "    'estimator__max_depth': [None, 5, 10, 20],  # Profundidad máxima del árbol a probar\n",
    "    'estimator__min_samples_split': [2, 5, 10],  # Número mínimo de muestras requeridas para dividir un nodo interno\n",
    "    'estimator__min_samples_leaf': [1, 2, 4]  # Número mínimo de muestras requeridas en un nodo hoja\n",
    "}\n",
    "\n",
    "param_grid_gradient_boosting = {\n",
    "    'estimator' : [GradientBoostingRegressor()],\n",
    "    'estimator__n_estimators': [100, 200, 500],  # Número de estimadores a probar\n",
    "    'estimator__learning_rate': [0.1, 0.01],  # Tasa de aprendizaje a probar\n",
    "    'estimator__max_depth': [3, 5, 10],  # Profundidad máxima del árbol base a probar\n",
    "    'estimator__subsample': [0.8, 1.0],  # Proporción de muestras utilizadas para entrenar cada árbol\n",
    "}\n",
    "\n",
    "param_grid_adaboost = {\n",
    "    'estimator' : [AdaBoostRegressor()],\n",
    "    'estimator__n_estimators': [50, 100, 200],  # Número de estimadores a probar\n",
    "    'estimator__learning_rate': [0.1, 0.01, 0.005],  # Tasa de aprendizaje a probar\n",
    "    'estimator__loss': ['linear', 'square', 'exponential']  # Función de pérdida a probar\n",
    "}\n",
    "\n",
    "param_grid_xgboost = {\n",
    "    'estimator' : [XGBRegressor()],\n",
    "    'estimator__n_estimators': [100, 200, 500],  # Número de árboles a probar\n",
    "    'estimator__learning_rate': [0.1, 0.01, 0.001],  # Tasa de aprendizaje a probar\n",
    "    'estimator__max_depth': [3, 5, 10],  # Profundidad máxima del árbol a probar\n",
    "    'estimator__subsample': [0.8, 1.0],  # Proporción de muestras utilizadas para entrenar cada árbol\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear espacio de búsqueda 1\n",
    "search_space = [\n",
    "    param_grid_linear_regression,\n",
    "    param_grid_ridge,\n",
    "    param_grid_lasso,\n",
    "    # param_grid_elasticnet,\n",
    "    # param_grid_svr,\n",
    "    param_grid_decision_tree,\n",
    "    param_grid_random_forest,\n",
    "    param_grid_gradient_boosting,\n",
    "    # param_grid_adaboost,\n",
    "    param_grid_xgboost # El mejor para el primer modelo\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear espacio de búsqueda 2\n",
    "search_space2 = [\n",
    "    # param_grid_linear_regression,\n",
    "    # param_grid_ridge,\n",
    "    # param_grid_lasso,\n",
    "    param_grid_elasticnet,\n",
    "    param_grid_svr, # Mejor luego quitarlo\n",
    "    # param_grid_decision_tree,\n",
    "    # param_grid_random_forest,\n",
    "    # param_grid_gradient_boosting,\n",
    "    param_grid_adaboost,\n",
    "    # param_grid_xgboost # El mejor para el primer modelo\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle = True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar la cross-validation\n",
    "gridsearch1 = GridSearchCV(\n",
    "    estimator = pipe,  # Pipeline\n",
    "    param_grid = search_space,  # Espacio de búsqueda\n",
    "    cv = cv,  # Validación cruzada\n",
    "    verbose=3,\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar la cross-validation\n",
    "gridsearch2 = GridSearchCV(\n",
    "    estimator = pipe,  # Pipeline\n",
    "    param_grid = search_space2,  # Espacio de búsqueda\n",
    "    cv = cv,  # Validación cruzada\n",
    "    verbose=3,\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos VotingRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_model1 = gridsearch1\n",
    "grid_model2 = gridsearch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 256 candidates, totalling 1280 fits\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=0, shuffle=True),\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('estimator', Ridge())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'estimator': [ElasticNet()],\n",
       "                          'estimator__alpha': [0.1, 1.0, 10.0],\n",
       "                          'estimator__l1_ratio': [0.25, 0.5, 0.75],\n",
       "                          'estimator__max_iter': [1000, 2000, 5000]},\n",
       "                         {'estimator': [SVR(epsilon=0.01)],\n",
       "                          'estimator__C': [0.1, 1.0, 10.0],\n",
       "                          'estimator__epsilon': [0.01, 0.1, 1.0],\n",
       "                          'estimator__kernel': ['linear', 'poly', 'rbf',\n",
       "                                                'sigmoid']},\n",
       "                         {'estimator': [AdaBoostRegressor()],\n",
       "                          'estimator__learning_rate': [0.1, 0.01, 0.005],\n",
       "                          'estimator__loss': ['linear', 'square',\n",
       "                                              'exponential'],\n",
       "                          'estimator__n_estimators': [50, 100, 200]}],\n",
       "             scoring='neg_mean_squared_error', verbose=3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamos la búsqueda\n",
    "gridsearch1.fit(X_train, y_train)\n",
    "gridsearch2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimator': XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, gamma=None,\n",
      "             gpu_id=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
      "             max_leaves=None, min_child_weight=None, missing=nan,\n",
      "             monotone_constraints=None, n_estimators=200, n_jobs=None,\n",
      "             num_parallel_tree=None, predictor=None, random_state=None,\n",
      "             reg_alpha=None, reg_lambda=None, ...), 'estimator__learning_rate': 0.1, 'estimator__max_depth': 5, 'estimator__n_estimators': 200, 'estimator__subsample': 0.8}\n",
      "Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('estimator',\n",
      "                 XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "                              colsample_bylevel=1, colsample_bynode=1,\n",
      "                              colsample_bytree=1, early_stopping_rounds=None,\n",
      "                              enable_categorical=False, eval_metric=None,\n",
      "                              gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "                              importance_type=None, interaction_constraints='',\n",
      "                              learning_rate=0.1, max_bin=256,\n",
      "                              max_cat_to_onehot=4, max_delta_step=0,\n",
      "                              max_depth=5, max_leaves=0, min_child_weight=1,\n",
      "                              missing=nan, monotone_constraints='()',\n",
      "                              n_estimators=200, n_jobs=0, num_parallel_tree=1,\n",
      "                              predictor='auto', random_state=0, reg_alpha=0,\n",
      "                              reg_lambda=1, ...))])\n"
     ]
    }
   ],
   "source": [
    "# Obtener los mejores hiperparámetros y el mejor modelo\n",
    "# best_params = gridsearch1.best_params_\n",
    "# best_model = gridsearch2.best_estimator_\n",
    "# print(best_params)\n",
    "# print(best_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los mejores modelos de GridSearch\n",
    "best_model1 = grid_model1.best_estimator_\n",
    "best_model2 = grid_model2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el ensemble a partir de los mejores modelos\n",
    "ensemble = VotingRegressor([('model1', best_model1), ('model2', best_model2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingRegressor(estimators=[('model1',\n",
       "                             Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                             ('estimator',\n",
       "                                              XGBRegressor(base_score=0.5,\n",
       "                                                           booster='gbtree',\n",
       "                                                           callbacks=None,\n",
       "                                                           colsample_bylevel=1,\n",
       "                                                           colsample_bynode=1,\n",
       "                                                           colsample_bytree=1,\n",
       "                                                           early_stopping_rounds=None,\n",
       "                                                           enable_categorical=False,\n",
       "                                                           eval_metric=None,\n",
       "                                                           gamma=0, gpu_id=-1,\n",
       "                                                           grow_policy='depthwise',\n",
       "                                                           importance_type=None,\n",
       "                                                           inter...\n",
       "                                                           learning_rate=0.1,\n",
       "                                                           max_bin=256,\n",
       "                                                           max_cat_to_onehot=4,\n",
       "                                                           max_delta_step=0,\n",
       "                                                           max_depth=5,\n",
       "                                                           max_leaves=0,\n",
       "                                                           min_child_weight=1,\n",
       "                                                           missing=nan,\n",
       "                                                           monotone_constraints='()',\n",
       "                                                           n_estimators=200,\n",
       "                                                           n_jobs=0,\n",
       "                                                           num_parallel_tree=1,\n",
       "                                                           predictor='auto',\n",
       "                                                           random_state=0,\n",
       "                                                           reg_alpha=0,\n",
       "                                                           reg_lambda=1, ...))])),\n",
       "                            ('model2',\n",
       "                             Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                             ('estimator',\n",
       "                                              SVR(epsilon=0.01))]))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir los valores de salida para los datos de prueba utilizando el mejor modelo\n",
    "y_pred = ensemble.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error cuadrático medio: 0.0060512554079105605\n",
      "Raíz del error cuadrático medio: 0.07778981557961531\n",
      "Error absoluto medio: 0.052410830791147694\n",
      "R2: 0.43384654376451226\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el rendimiento del mejor modelo\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print('Error cuadrático medio:', mse)\n",
    "print('Raíz del error cuadrático medio:', np.sqrt(mse))\n",
    "print('Error absoluto medio:', mean_absolute_error(y_test, y_pred))\n",
    "print('R2:', r2_score(y_test, y_pred))\n",
    "#print('Mejores hiperparámetros:', best_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraemos los mejores hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estimators': [('model1', Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('estimator',\n",
      "                 XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "                              colsample_bylevel=1, colsample_bynode=1,\n",
      "                              colsample_bytree=1, early_stopping_rounds=None,\n",
      "                              enable_categorical=False, eval_metric=None,\n",
      "                              gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "                              importance_type=None, interaction_constraints='',\n",
      "                              learning_rate=0.1, max_bin=256,\n",
      "                              max_cat_to_onehot=4, max_delta_step=0,\n",
      "                              max_depth=5, max_leaves=0, min_child_weight=1,\n",
      "                              missing=nan, monotone_constraints='()',\n",
      "                              n_estimators=200, n_jobs=0, num_parallel_tree=1,\n",
      "                              predictor='auto', random_state=0, reg_alpha=0,\n",
      "                              reg_lambda=1, ...))])), ('model2', Pipeline(steps=[('scaler', StandardScaler()), ('estimator', SVR(epsilon=0.01))]))], 'n_jobs': None, 'verbose': False, 'weights': None, 'model1': Pipeline(steps=[('scaler', StandardScaler()),\n",
      "                ('estimator',\n",
      "                 XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "                              colsample_bylevel=1, colsample_bynode=1,\n",
      "                              colsample_bytree=1, early_stopping_rounds=None,\n",
      "                              enable_categorical=False, eval_metric=None,\n",
      "                              gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "                              importance_type=None, interaction_constraints='',\n",
      "                              learning_rate=0.1, max_bin=256,\n",
      "                              max_cat_to_onehot=4, max_delta_step=0,\n",
      "                              max_depth=5, max_leaves=0, min_child_weight=1,\n",
      "                              missing=nan, monotone_constraints='()',\n",
      "                              n_estimators=200, n_jobs=0, num_parallel_tree=1,\n",
      "                              predictor='auto', random_state=0, reg_alpha=0,\n",
      "                              reg_lambda=1, ...))]), 'model2': Pipeline(steps=[('scaler', StandardScaler()), ('estimator', SVR(epsilon=0.01))]), 'model1__memory': None, 'model1__steps': [('scaler', StandardScaler()), ('estimator', XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "             early_stopping_rounds=None, enable_categorical=False,\n",
      "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "             importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
      "             max_delta_step=0, max_depth=5, max_leaves=0, min_child_weight=1,\n",
      "             missing=nan, monotone_constraints='()', n_estimators=200, n_jobs=0,\n",
      "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
      "             reg_lambda=1, ...))], 'model1__verbose': False, 'model1__scaler': StandardScaler(), 'model1__estimator': XGBRegressor(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "             early_stopping_rounds=None, enable_categorical=False,\n",
      "             eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "             importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
      "             max_delta_step=0, max_depth=5, max_leaves=0, min_child_weight=1,\n",
      "             missing=nan, monotone_constraints='()', n_estimators=200, n_jobs=0,\n",
      "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
      "             reg_lambda=1, ...), 'model1__scaler__copy': True, 'model1__scaler__with_mean': True, 'model1__scaler__with_std': True, 'model1__estimator__objective': 'reg:squarederror', 'model1__estimator__base_score': 0.5, 'model1__estimator__booster': 'gbtree', 'model1__estimator__callbacks': None, 'model1__estimator__colsample_bylevel': 1, 'model1__estimator__colsample_bynode': 1, 'model1__estimator__colsample_bytree': 1, 'model1__estimator__early_stopping_rounds': None, 'model1__estimator__enable_categorical': False, 'model1__estimator__eval_metric': None, 'model1__estimator__gamma': 0, 'model1__estimator__gpu_id': -1, 'model1__estimator__grow_policy': 'depthwise', 'model1__estimator__importance_type': None, 'model1__estimator__interaction_constraints': '', 'model1__estimator__learning_rate': 0.1, 'model1__estimator__max_bin': 256, 'model1__estimator__max_cat_to_onehot': 4, 'model1__estimator__max_delta_step': 0, 'model1__estimator__max_depth': 5, 'model1__estimator__max_leaves': 0, 'model1__estimator__min_child_weight': 1, 'model1__estimator__missing': nan, 'model1__estimator__monotone_constraints': '()', 'model1__estimator__n_estimators': 200, 'model1__estimator__n_jobs': 0, 'model1__estimator__num_parallel_tree': 1, 'model1__estimator__predictor': 'auto', 'model1__estimator__random_state': 0, 'model1__estimator__reg_alpha': 0, 'model1__estimator__reg_lambda': 1, 'model1__estimator__sampling_method': 'uniform', 'model1__estimator__scale_pos_weight': 1, 'model1__estimator__subsample': 0.8, 'model1__estimator__tree_method': 'exact', 'model1__estimator__validate_parameters': 1, 'model1__estimator__verbosity': None, 'model2__memory': None, 'model2__steps': [('scaler', StandardScaler()), ('estimator', SVR(epsilon=0.01))], 'model2__verbose': False, 'model2__scaler': StandardScaler(), 'model2__estimator': SVR(epsilon=0.01), 'model2__scaler__copy': True, 'model2__scaler__with_mean': True, 'model2__scaler__with_std': True, 'model2__estimator__C': 1.0, 'model2__estimator__cache_size': 200, 'model2__estimator__coef0': 0.0, 'model2__estimator__degree': 3, 'model2__estimator__epsilon': 0.01, 'model2__estimator__gamma': 'scale', 'model2__estimator__kernel': 'rbf', 'model2__estimator__max_iter': -1, 'model2__estimator__shrinking': True, 'model2__estimator__tol': 0.001, 'model2__estimator__verbose': False}\n"
     ]
    }
   ],
   "source": [
    "# Extraemos los mejores hiperparámetros de best_model\n",
    "best_model_params = ensemble.get_params()\n",
    "print(best_model_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representamos la importancia de cada feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VotingRegressor' object has no attribute 'feature_importances_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4936\\4176678492.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Extraemos la feature importance del best model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfeature_importance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensemble\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfeature_importance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfeature_importance\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mfeature_importance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msorted_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_importance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'VotingRegressor' object has no attribute 'feature_importances_'"
     ]
    }
   ],
   "source": [
    "# Extraemos la feature importance del best model\n",
    "feature_importance = best_model.named_steps['estimator'].feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "feature_names = X.columns\n",
    "feature_names = feature_names[sorted_idx]\n",
    "feature_importance = feature_importance[sorted_idx]\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "# Representamos la importancia de las features\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.barh(pos, feature_importance, align='center')\n",
    "plt.yticks(pos, feature_names)\n",
    "plt.xlabel('Importancia relativa')\n",
    "plt.title('Importancia de las features')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraemos todas las métricas para este modelo en formato csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos las métricas de evaluación para todos los modelos del pipeline en formato dataframe y lo pasamos a csv\n",
    "cv_results = pd.DataFrame(gridsearch.cv_results_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos todas las métricas de evaluación del modelo en formato dataframe y luego lo pasamos a csv\n",
    "metricas_votingregressor = pd.DataFrame({'MSE' : [mse], 'RMSE' : [rmse], 'MAE' : [mae], 'R2' : [r2]})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardamos el mejor modelo en un archivo .pickle\n",
    "#### (Pendiente de hacer función)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo se ha guardado exitosamente en ../output/models/primer_procesado_cuarto_modelo_ensemble.pickle.\n"
     ]
    }
   ],
   "source": [
    "nombre_archivo = '../output/models/primer_procesado_cuarto_modelo_ensemble.pickle'\n",
    "modelo = ensemble\n",
    "import pickle\n",
    "try:\n",
    "    with open(nombre_archivo, 'wb') as archivo:\n",
    "        pickle.dump(modelo, archivo)\n",
    "    print(f\"El modelo se ha guardado exitosamente en {nombre_archivo}.\")\n",
    "except IOError:\n",
    "    print(\"Error: No se pudo guardar el modelo. Permiso denegado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_datos(nombre_archivo, data):\n",
    "    '''Guarda el dataframe data en un archivo csv con nombre nombre_archivo\n",
    "    en la carpeta data/processed'''\n",
    "    ruta_archivo = '../output/reports'\n",
    "    data.to_csv(ruta_archivo + \"/\" + nombre_archivo, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos los datos con las métricas en un csv\n",
    "ruta_archivo = './output/reports/'\n",
    "csv_datos(\"metricas_primer_procesado_cuarto_modelo_xgboost.csv\", metricas_votingregressor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
